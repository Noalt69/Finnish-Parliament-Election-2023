{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\An\\AppData\\Local\\Temp\\ipykernel_26944\\4030584891.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2746920114.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    df_elected = pd.read_csv(\"C:\\Users\\An\\Aalto University\\Cao Duong - ECONOMETRICS-CAPSTONE\\submission file\\elected.csv\", skiprows=1, header=None)\u001b[0m\n\u001b[1;37m                                                                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "df_elected = pd.read_csv(\"C:\\Users\\An\\Aalto University\\Cao Duong - ECONOMETRICS-CAPSTONE\\submission file\\elected.csv\", skiprows=1, header=None)\n",
    "\n",
    "df_capstone = pd.read_csv(\"Data_Capstone_cvs.csv\", skiprows=1, header=None)\n",
    "\n",
    "num_cols_elected = df_elected.shape[1]\n",
    "df_elected.columns = [f\"v{i+1}\" for i in range(num_cols_elected)]\n",
    "\n",
    "num_cols_capstone = df_capstone.shape[1]\n",
    "df_capstone.columns = [f\"v{i+1}\" for i in range(num_cols_capstone)]\n",
    "\n",
    "\n",
    "df_merged = pd.merge(df_capstone, df_elected, on=[\"v1\", \"v2\", \"v3\"], how=\"outer\", suffixes=(\"\", \"_elected\"))\n",
    "\n",
    "\n",
    "df_merged['elected'] = np.where(df_merged[\"v4_elected\"].notna(), 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if df_merged.index.max() >= 927:  \n",
    "    df_merged.loc[927, \"v40\"] = \"600\"\n",
    "\n",
    "for i in range(1, 43):  # 1 to 42\n",
    "    col = f\"v{i}\"\n",
    "    if col in df_merged.columns and df_merged[col].dtype == object:\n",
    "        df_merged[col] = df_merged[col].astype(str).str.replace(\"'\", \"\", regex=True)\n",
    "        df_merged[col] = df_merged[col].astype(str).str.replace(\",\", \".\", regex=True)\n",
    "\n",
    "# Convert columns to numeric\n",
    "cols_to_destring_1 = [f\"v{i}\" for i in range(10, 26)]  # v10 to v25\n",
    "cols_to_destring_2 = [\"v28\", \"v31\", \"v34\", \"v37\", \"v40\"]\n",
    "\n",
    "for col in (cols_to_destring_1 + cols_to_destring_2):\n",
    "    if col in df_merged.columns:\n",
    "        df_merged[col] = pd.to_numeric(df_merged[col], errors='coerce')\n",
    "\n",
    "\n",
    "# ### Rename and drop columns\n",
    "\n",
    "# Create a dictionary of old->new column names\n",
    "rename_dict = {\n",
    "    \"v1\": \"candidate_number\",\n",
    "    \"v2\": \"first_name\",\n",
    "    \"v3\": \"last_name\",\n",
    "    \"v4\": \"title\",\n",
    "    \"v5\": \"municipality\",\n",
    "    \"v6\": \"party\",\n",
    "    \"v7\": \"arrival_day\",\n",
    "    \"v8\": \"date_modified\",\n",
    "    \"v9\": \"support_group\",\n",
    "    \"v10\": \"total_expenses\",\n",
    "    \"v11\": \"newspaper\",\n",
    "    \"v12\": \"radio\",\n",
    "    \"v13\": \"TV\",\n",
    "    \"v14\": \"info_network\",\n",
    "    \"v15\": \"other_platform\",\n",
    "    \"v16\": \"outdoor_ad\",\n",
    "    \"v17\": \"purchases\",\n",
    "    \"v18\": \"ad_design\",\n",
    "    \"v19\": \"event\",\n",
    "    \"v20\": \"aquisition_cost\",\n",
    "    \"v21\": \"other_charges\",\n",
    "    \"v22\": \"total_funding\",\n",
    "    \"v23\": \"own_resources\",\n",
    "    \"v24\": \"loans\",\n",
    "    \"v25\": \"individual_support\",\n",
    "    \"v28\": \"aid\",\n",
    "    \"v31\": \"party_support\",\n",
    "    \"v34\": \"party_support_association\",\n",
    "    \"v37\": \"other_sources_support\",\n",
    "    \"v40\": \"intermediated_aid\"\n",
    "}\n",
    "df_merged.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Example of generating ex_indi_min1500 from v26 == \"X\"\n",
    "if \"v26\" in df_merged.columns:\n",
    "    df_merged[\"ex_indi_min1500\"] = df_merged[\"v26\"].astype(str).eq(\"X\")\n",
    "\n",
    "# Drop columns no longer needed\n",
    "cols_to_drop = [\n",
    "    \"v26\", \"v27\", \"v29\", \"v30\", \"v32\", \"v33\", \"v35\", \"v36\", \"v38\", \"v39\", \"v41\", \"v42\",\n",
    "    \"title\", \"arrival_day\", \"date_modified\", \"support_group\"\n",
    "]\n",
    "df_merged.drop(columns=[c for c in cols_to_drop if c in df_merged.columns], inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Column reorder and correlation checks\n",
    "\n",
    "# Reorder columns so that total_funding appears before total_expenses\n",
    "cols = list(df_merged.columns)\n",
    "if \"total_funding\" in cols and \"total_expenses\" in cols:\n",
    "    cols.remove(\"total_funding\")\n",
    "    idx = cols.index(\"total_expenses\")\n",
    "    cols.insert(idx, \"total_funding\")\n",
    "df_merged = df_merged[cols]\n",
    "\n",
    "# Correlation among selected variables\n",
    "vars_for_corr = [\"elected\", \"newspaper\", \"radio\", \"TV\", \"info_network\", \n",
    "                 \"other_platform\", \"outdoor_ad\", \"purchases\", \"ad_design\", \"event\"]\n",
    "corr_matrix = df_merged[vars_for_corr].corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Logistic regression on total_funding\n",
    "\n",
    "df_model_1 = df_merged.dropna(subset=[\"elected\", \"total_funding\"]).copy()\n",
    "\n",
    "# Build design matrices\n",
    "X = sm.add_constant(df_model_1[\"total_funding\"])  # Add intercept\n",
    "y = df_model_1[\"elected\"]\n",
    "\n",
    "# Fit logistic regression\n",
    "logit_model_1 = sm.Logit(y, X).fit(disp=0)\n",
    "print(logit_model_1.summary())\n",
    "\n",
    "# Marginal (partial) effects for total_funding\n",
    "# statsmodels allows get_margeff for logistic models:\n",
    "mfx = logit_model_1.get_margeff()\n",
    "print(mfx.summary())\n",
    "\n",
    "# Predicted probabilities\n",
    "df_model_1[\"p\"] = logit_model_1.predict(X)\n",
    "\n",
    "# Plot predicted probabilities against total_funding\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df_model_1[\"total_funding\"], df_model_1[\"p\"], alpha=0.5, label=\"Predicted Probabilities\")\n",
    "sns.regplot(x=\"total_funding\", y=\"p\", data=df_model_1, \n",
    "            scatter=False, lowess=True, color=\"blue\", label=\"Lowess\")\n",
    "plt.xlabel(\"Total Funding\")\n",
    "plt.ylabel(\"Predicted Probability of Elected\")\n",
    "plt.title(\"Linearity of Logit (Predicted Probability vs. Total Funding)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Illustrative Stepwise by AIC\n",
    "\n",
    "# %%\n",
    "predictors = [\"aid\", \"other_sources_support\", \"own_resources\", \n",
    "              \"party_support_association\", \"intermediated_aid\",\n",
    "              \"loans\", \"individual_support\"]\n",
    "\n",
    "df_model_2 = df_merged.dropna(subset=[\"elected\"] + predictors).copy()\n",
    "y = df_model_2[\"elected\"]\n",
    "\n",
    "best_model = None\n",
    "best_aic = np.inf\n",
    "current_predictors = []\n",
    "\n",
    "# Forward stepwise approach\n",
    "for var in predictors:\n",
    "    test_model_predictors = current_predictors + [var]\n",
    "    X_test = sm.add_constant(df_model_2[test_model_predictors])\n",
    "    try:\n",
    "        fit_test = sm.Logit(y, X_test).fit(disp=0)\n",
    "        if fit_test.aic < best_aic:\n",
    "            best_aic = fit_test.aic\n",
    "            best_model = fit_test\n",
    "            current_predictors = test_model_predictors\n",
    "    except:\n",
    "        # If the model fails to converge for any reason, skip\n",
    "        continue\n",
    "    \n",
    "    print(f\"Current model predictors: {test_model_predictors}\")\n",
    "    print(f\"AIC: {fit_test.aic}\")\n",
    "    print(f\"Best model so far: {current_predictors}, AIC = {best_aic}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "print(\"Final model predictors:\", current_predictors)\n",
    "print(\"Final model AIC:\", best_aic)\n",
    "\n",
    "if best_model is not None:\n",
    "    print(best_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Compare Probit and Logit with multiple predictors\n",
    "\n",
    "# %%\n",
    "predictors_ad = [\"newspaper\", \"radio\", \"TV\", \"info_network\", \n",
    "                 \"other_platform\", \"outdoor_ad\", \"purchases\", \"event\"]\n",
    "df_model_3 = df_merged.dropna(subset=[\"elected\"] + predictors_ad)\n",
    "\n",
    "X_ad = sm.add_constant(df_model_3[predictors_ad])\n",
    "y_ad = df_model_3[\"elected\"]\n",
    "\n",
    "# Probit\n",
    "probit_model = sm.Probit(y_ad, X_ad).fit(disp=0)\n",
    "print(\"Probit model:\\n\", probit_model.summary())\n",
    "\n",
    "# Logit\n",
    "logit_model_2 = sm.Logit(y_ad, X_ad).fit(disp=0)\n",
    "print(\"\\nLogit model:\\n\", logit_model_2.summary())\n",
    "\n",
    "# Marginal effects for Logit\n",
    "mfx_logit_2 = logit_model_2.get_margeff()\n",
    "print(\"\\nMarginal effects (Logit):\\n\", mfx_logit_2.summary())\n",
    "\n",
    "# Pairwise correlation\n",
    "corr_matrix_ad = df_model_3[[\"elected\"] + predictors_ad].corr()\n",
    "print(\"\\nPairwise correlations:\\n\", corr_matrix_ad)\n",
    "\n",
    "# Histogram example\n",
    "df_model_3[\"newspaper\"].hist(bins=20)\n",
    "plt.title(\"Histogram of 'newspaper'\")\n",
    "plt.xlabel(\"Spending\\Value for Newspaper\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Heatmap of correlation matrix\n",
    "\n",
    "# %%\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix_ad, annot=True, cmap=\"coolwarm\", fmt=\".3f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Residual diagnostics\n",
    "\n",
    "# %%\n",
    "df_model_3[\"resid\"] = logit_model_2.resid_linear  # Linear predictors residual\n",
    "plt.figure()\n",
    "sns.histplot(df_model_3[\"resid\"], kde=True)\n",
    "plt.title(\"Histogram of Residuals\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.show()\n",
    "\n",
    "# Normal Q-Q plot\n",
    "sm.qqplot(df_model_3[\"resid\"], line='45', fit=True)\n",
    "plt.title(\"Normal Q-Q of Residuals\")\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test (like swilk)\n",
    "from scipy.stats import shapiro\n",
    "stat, p_value = shapiro(df_model_3[\"resid\"].dropna())\n",
    "print(f\"Shapiro-Wilk Test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Subgroup analysis by region\n",
    "\n",
    "# %%\n",
    "df_merged[\"region\"] = np.nan\n",
    "\n",
    "# Example logic: \"gen region = substr(municipality, 1, strpos(municipality, \" \") - 1)\"\n",
    "# We'll try a Python version if municipality has spaces\n",
    "def extract_region(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    parts = str(x).split()\n",
    "    return parts[0] if len(parts) > 0 else np.nan\n",
    "\n",
    "df_merged[\"region\"] = df_merged[\"municipality\"].apply(extract_region)\n",
    "\n",
    "# Remove hyphens\n",
    "df_merged[\"region\"] = df_merged[\"region\"].str.replace(\"-\", \"\", regex=True)\n",
    "\n",
    "# Drop municipality if desired\n",
    "df_merged.drop(columns=[\"municipality\"], inplace=True)\n",
    "\n",
    "# Unique levels of region\n",
    "regions = df_merged[\"region\"].dropna().unique()\n",
    "\n",
    "for reg in regions:\n",
    "    df_sub = df_merged.loc[df_merged[\"region\"] == reg].dropna(\n",
    "        subset=[\"elected\", \"newspaper\", \"radio\", \"TV\", \n",
    "                \"info_network\", \"outdoor_ad\", \"purchases\", \"event\"]\n",
    "    )\n",
    "    if df_sub.empty:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    X_sub = sm.add_constant(df_sub[[\"newspaper\", \"radio\", \"TV\", \n",
    "                                   \"info_network\", \"outdoor_ad\", \n",
    "                                   \"purchases\", \"event\"]])\n",
    "    y_sub = df_sub[\"elected\"]\n",
    "    \n",
    "    try:\n",
    "        fit_sub = sm.Logit(y_sub, X_sub).fit(disp=0)\n",
    "        print(f\"Region: {reg}\")\n",
    "        print(fit_sub.summary())\n",
    "        mfx_sub = fit_sub.get_margeff()\n",
    "        print(\"Marginal effects:\\n\", mfx_sub.summary())\n",
    "        print(\"-\"*60)\n",
    "    except:\n",
    "        print(f\"Model failed for region: {reg}\")\n",
    "        print(\"-\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
